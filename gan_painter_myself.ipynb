{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10b25ff",
   "metadata": {},
   "source": [
    "# I'm Something of a Painter Myself - CycleGAN Implementation\n",
    "\n",
    "## 1. Introduction\n",
    "This notebook implements a CycleGAN to translate photos into Monet-style paintings.\n",
    "Competition: [I'm Something of a Painter Myself](https://www.kaggle.com/competitions/gan-getting-started/overview)\n",
    "\n",
    "**Strategy**:\n",
    "1.  **Data**: Load from Google Drive (Zip file) for faster IO.\n",
    "2.  **Model**: CycleGAN (ResNet Generator + PatchGAN Discriminator).\n",
    "3.  **Framework**: PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6766ccae",
   "metadata": {},
   "source": [
    "## 2. Data Pipeline\n",
    "\n",
    "### Data Loading Strategy\n",
    "We will use **Google Drive** to store the dataset.\n",
    "1.  Download `gan-getting-started.zip` from Kaggle to your local machine.\n",
    "2.  Upload the zip file to your Google Drive (e.g., in a folder named `kaggle_gan`).\n",
    "3.  Mount Drive in Colab.\n",
    "4.  Copy the zip to the local Colab environment (`/content/`) and unzip it. This is much faster than reading files directly from Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "# 1. Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Define paths\n",
    "# CHANGE THIS PATH to where you uploaded the zip file in your Drive\n",
    "drive_zip_path = '/content/drive/MyDrive/kaggle_gan/gan-getting-started.zip' \n",
    "local_zip_path = '/content/gan-getting-started.zip'\n",
    "dataset_dir = '/content/dataset'\n",
    "\n",
    "# 3. Copy and Unzip\n",
    "if not os.path.exists(dataset_dir):\n",
    "    print(\"Copying zip file...\")\n",
    "    shutil.copy(drive_zip_path, local_zip_path)\n",
    "    \n",
    "    print(\"Unzipping...\")\n",
    "    shutil.unpack_archive(local_zip_path, dataset_dir)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Dataset already exists.\")\n",
    "\n",
    "# 4. Verify\n",
    "print(f\"Photos: {len(os.listdir(os.path.join(dataset_dir, 'photo_jpg')))}\")\n",
    "print(f\"Monet: {len(os.listdir(os.path.join(dataset_dir, 'monet_jpg')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ae0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_monet, root_photo, transform=None):\n",
    "        self.root_monet = root_monet\n",
    "        self.root_photo = root_photo\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.monet_images = os.listdir(root_monet)\n",
    "        self.photo_images = os.listdir(root_photo)\n",
    "        self.length_dataset = max(len(self.monet_images), len(self.photo_images))\n",
    "        self.monet_len = len(self.monet_images)\n",
    "        self.photo_len = len(self.photo_images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        monet_img = self.monet_images[index % self.monet_len]\n",
    "        photo_img = self.photo_images[index % self.photo_len]\n",
    "\n",
    "        monet_path = os.path.join(self.root_monet, monet_img)\n",
    "        photo_path = os.path.join(self.root_photo, photo_img)\n",
    "\n",
    "        monet_img = np.array(Image.open(monet_path).convert(\"RGB\"))\n",
    "        photo_img = np.array(Image.open(photo_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=monet_img, image0=photo_img)\n",
    "            monet_img = augmentations[\"image\"]\n",
    "            photo_img = augmentations[\"image0\"]\n",
    "\n",
    "        return monet_img, photo_img\n",
    "\n",
    "# Note: We will need albumentations or torchvision transforms. \n",
    "# Here is a simple torchvision version for starter:\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Simplified Dataset for torchvision (without albumentations for now)\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, root_monet, root_photo, transform=None):\n",
    "        self.monet_files = [os.path.join(root_monet, f) for f in os.listdir(root_monet)]\n",
    "        self.photo_files = [os.path.join(root_photo, f) for f in os.listdir(root_photo)]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(len(self.monet_files), len(self.photo_files))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        monet_img = Image.open(self.monet_files[idx % len(self.monet_files)]).convert(\"RGB\")\n",
    "        photo_img = Image.open(self.photo_files[idx % len(self.photo_files)]).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            monet_img = self.transform(monet_img)\n",
    "            photo_img = self.transform(photo_img)\n",
    "            \n",
    "        return monet_img, photo_img\n",
    "\n",
    "# Create DataLoader\n",
    "# dataset = SimpleDataset(\n",
    "#     root_monet=os.path.join(dataset_dir, 'monet_jpg'),\n",
    "#     root_photo=os.path.join(dataset_dir, 'photo_jpg'),\n",
    "#     transform=transform\n",
    "# )\n",
    "# loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d2d6b",
   "metadata": {},
   "source": [
    "## 3. Model Architecture (CycleGAN)\n",
    "\n",
    "We need:\n",
    "1.  **Generator**: ResNet-based (9 blocks for 256x256 images).\n",
    "2.  **Discriminator**: PatchGAN (70x70 PatchGAN).\n",
    "3.  **Weights Initialization**: Normal distribution with mean 0.0, std 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels=3, num_residuals=9):\n",
    "        super(Generator, self).__init__()\n",
    "        # Initial Convolution\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(img_channels, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # Residual Blocks\n",
    "        for _ in range(num_residuals):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # Output Layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, img_channels, 7),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], 4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, feature, 4, stride=1 if feature == features[-1] else 2, padding=1, padding_mode=\"reflect\"),\n",
    "                    nn.InstanceNorm2d(feature),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                )\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        layers.append(nn.Conv2d(in_channels, 1, 4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.initial(x)) if False else self.model(self.initial(x)) # PatchGAN output\n",
    "\n",
    "def test():\n",
    "    img_channels = 3\n",
    "    img_size = 256\n",
    "    x = torch.randn((2, img_channels, img_size, img_size))\n",
    "    gen = Generator(img_channels, 9)\n",
    "    disc = Discriminator(img_channels)\n",
    "    print(gen(x).shape)\n",
    "    print(disc(x).shape)\n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a2002",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "This section defines the training loop.\n",
    "*   **Losses**: Adversarial (MSE), Cycle (L1), Identity (L1).\n",
    "*   **Optimizers**: Adam.\n",
    "*   **Loop**: Iterate through epochs, update G and D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917bf5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 30 # Increase this for better results\n",
    "LAMBDA_CYCLE = 10\n",
    "LAMBDA_IDENTITY = 0.5\n",
    "\n",
    "# Initialize Models\n",
    "gen_Z = Generator(img_channels=3, num_residuals=9).to(device) # Photo -> Monet\n",
    "gen_P = Generator(img_channels=3, num_residuals=9).to(device) # Monet -> Photo\n",
    "disc_Z = Discriminator(in_channels=3).to(device) # Classify Monet\n",
    "disc_P = Discriminator(in_channels=3).to(device) # Classify Photo\n",
    "\n",
    "# Optimizers\n",
    "opt_gen = optim.Adam(\n",
    "    list(gen_Z.parameters()) + list(gen_P.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "opt_disc = optim.Adam(\n",
    "    list(disc_Z.parameters()) + list(disc_P.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "# Losses\n",
    "L1 = nn.L1Loss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Training Loop Skeleton\n",
    "def train_fn(disc_Z, disc_P, gen_Z, gen_P, loader, opt_disc, opt_gen, L1, mse):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for idx, (monet, photo) in enumerate(loop):\n",
    "        monet = monet.to(device)\n",
    "        photo = photo.to(device)\n",
    "\n",
    "        # Train Discriminators H and Z\n",
    "        # ... (Implementation needed) ...\n",
    "\n",
    "        # Train Generators H and Z\n",
    "        # ... (Implementation needed) ...\n",
    "\n",
    "        # Update progress bar\n",
    "        # loop.set_postfix(H_real=H_reals / (idx + 1), H_fake=H_fakes / (idx + 1))\n",
    "\n",
    "# Run Training\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     train_fn(disc_Z, disc_P, gen_Z, gen_P, loader, opt_disc, opt_gen, L1, mse)\n",
    "#     # Save Model Checkpoints\n",
    "#     # Save Sample Images"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
