{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10b25ff",
   "metadata": {},
   "source": [
    "# I'm Something of a Painter Myself - CycleGAN Implementation\n",
    "\n",
    "## 1. Introduction\n",
    "This notebook implements a CycleGAN to translate photos into Monet-style paintings.\n",
    "Competition: [I'm Something of a Painter Myself](https://www.kaggle.com/competitions/gan-getting-started/overview)\n",
    "\n",
    "**Strategy**:\n",
    "1.  **Data**: Load from Google Drive (Zip file) for faster IO.\n",
    "2.  **Model**: CycleGAN (ResNet Generator + PatchGAN Discriminator).\n",
    "3.  **Framework**: PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd28e3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6766ccae",
   "metadata": {},
   "source": [
    "## 2. Data Pipeline\n",
    "\n",
    "### Data Loading Strategy\n",
    "We will download the dataset directly from a shared Google Drive link.\n",
    "1.  Use `gdown` to download the zip file from the shared link.\n",
    "2.  Unzip it to the local environment.\n",
    "This method works in both Colab and local VS Code environments without requiring authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3985d6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading zip file to /content/gan-getting-started.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Wf8cZM1QboZamZDoL9hcuA8yIFUQtgEb\n",
      "From (redirected): https://drive.google.com/uc?id=1Wf8cZM1QboZamZDoL9hcuA8yIFUQtgEb&confirm=t&uuid=90809daa-a562-429b-be2a-f56b6cd82f74\n",
      "To: /content/gan-getting-started.zip\n",
      "100%|██████████| 385M/385M [00:08<00:00, 44.1MB/s] \n",
      "100%|██████████| 385M/385M [00:08<00:00, 44.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping...\n",
      "Done!\n",
      "Photos: 7038\n",
      "Monet: 300\n",
      "Done!\n",
      "Photos: 7038\n",
      "Monet: 300\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gdown\n",
    "\n",
    "# 1. Define paths and Google Drive File ID\n",
    "# Shared Link: https://drive.google.com/file/d/1Wf8cZM1QboZamZDoL9hcuA8yIFUQtgEb/view?usp=sharing\n",
    "file_id = '1Wf8cZM1QboZamZDoL9hcuA8yIFUQtgEb'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "# Use current directory or /content/ if it exists (Colab default)\n",
    "base_dir = '/content' if os.path.exists('/content') else '.'\n",
    "local_zip_path = os.path.join(base_dir, 'gan-getting-started.zip')\n",
    "dataset_dir = os.path.join(base_dir, 'dataset')\n",
    "\n",
    "# 2. Download and Unzip\n",
    "if not os.path.exists(dataset_dir):\n",
    "    print(f\"Downloading zip file to {local_zip_path}...\")\n",
    "    # gdown might need to be installed: pip install gdown\n",
    "    gdown.download(url, local_zip_path, quiet=False)\n",
    "    \n",
    "    print(\"Unzipping...\")\n",
    "    shutil.unpack_archive(local_zip_path, dataset_dir)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Dataset already exists.\")\n",
    "\n",
    "# 3. Verify\n",
    "if os.path.exists(dataset_dir):\n",
    "    photo_dir = os.path.join(dataset_dir, 'photo_jpg')\n",
    "    monet_dir = os.path.join(dataset_dir, 'monet_jpg')\n",
    "    if os.path.exists(photo_dir) and os.path.exists(monet_dir):\n",
    "        print(f\"Photos: {len(os.listdir(photo_dir))}\")\n",
    "        print(f\"Monet: {len(os.listdir(monet_dir))}\")\n",
    "    else:\n",
    "        print(\"Directories not found. Check unzip structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ae0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_monet, root_photo, transform=None):\n",
    "        self.root_monet = root_monet\n",
    "        self.root_photo = root_photo\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.monet_images = os.listdir(root_monet)\n",
    "        self.photo_images = os.listdir(root_photo)\n",
    "        self.length_dataset = max(len(self.monet_images), len(self.photo_images))\n",
    "        self.monet_len = len(self.monet_images)\n",
    "        self.photo_len = len(self.photo_images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        monet_img = self.monet_images[index % self.monet_len]\n",
    "        photo_img = self.photo_images[index % self.photo_len]\n",
    "\n",
    "        monet_path = os.path.join(self.root_monet, monet_img)\n",
    "        photo_path = os.path.join(self.root_photo, photo_img)\n",
    "\n",
    "        monet_img = np.array(Image.open(monet_path).convert(\"RGB\"))\n",
    "        photo_img = np.array(Image.open(photo_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.transform:\n",
    "            # Note: If using albumentations, the call is different.\n",
    "            # Here we assume standard torchvision transforms for simplicity in this starter\n",
    "            # For better results, consider using albumentations for augmentation\n",
    "            monet_img = Image.fromarray(monet_img)\n",
    "            photo_img = Image.fromarray(photo_img)\n",
    "            monet_img = self.transform(monet_img)\n",
    "            photo_img = self.transform(photo_img)\n",
    "\n",
    "        return monet_img, photo_img\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = ImageDataset(\n",
    "    root_monet=os.path.join(dataset_dir, 'monet_jpg'),\n",
    "    root_photo=os.path.join(dataset_dir, 'photo_jpg'),\n",
    "    transform=transform\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Visualization\n",
    "def show_sample(loader):\n",
    "    monet, photo = next(iter(loader))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    # Denormalize\n",
    "    monet = monet * 0.5 + 0.5\n",
    "    photo = photo * 0.5 + 0.5\n",
    "    \n",
    "    ax[0].imshow(monet[0].permute(1, 2, 0))\n",
    "    ax[0].set_title(\"Monet Style\")\n",
    "    ax[0].axis(\"off\")\n",
    "    \n",
    "    ax[1].imshow(photo[0].permute(1, 2, 0))\n",
    "    ax[1].set_title(\"Photo\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_sample(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d2d6b",
   "metadata": {},
   "source": [
    "## 3. Model Architecture (CycleGAN)\n",
    "\n",
    "We need:\n",
    "1.  **Generator**: ResNet-based (9 blocks for 256x256 images).\n",
    "2.  **Discriminator**: PatchGAN (70x70 PatchGAN).\n",
    "3.  **Weights Initialization**: Normal distribution with mean 0.0, std 0.02.\n",
    "4.  **Replay Buffer**: To store generated images for discriminator training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels=3, num_residuals=9):\n",
    "        super(Generator, self).__init__()\n",
    "        # Initial Convolution\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(img_channels, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # Residual Blocks\n",
    "        for _ in range(num_residuals):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # Output Layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, img_channels, 7),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], 4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, feature, 4, stride=1 if feature == features[-1] else 2, padding=1, padding_mode=\"reflect\"),\n",
    "                    nn.InstanceNorm2d(feature),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                )\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        layers.append(nn.Conv2d(in_channels, 1, 4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(self.initial(x))\n",
    "\n",
    "def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                nn.init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                nn.init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a2002",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "This section defines the training loop.\n",
    "*   **Losses**: Adversarial (MSE), Cycle (L1), Identity (L1).\n",
    "*   **Optimizers**: Adam.\n",
    "*   **Loop**: Iterate through epochs, update G and D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917bf5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 30 \n",
    "LAMBDA_CYCLE = 10\n",
    "LAMBDA_IDENTITY = 0.5\n",
    "\n",
    "# Initialize Models\n",
    "gen_Z = Generator(img_channels=3, num_residuals=9).to(device) # Photo -> Monet\n",
    "gen_P = Generator(img_channels=3, num_residuals=9).to(device) # Monet -> Photo\n",
    "disc_Z = Discriminator(in_channels=3).to(device) # Classify Monet\n",
    "disc_P = Discriminator(in_channels=3).to(device) # Classify Photo\n",
    "\n",
    "# Initialize Weights\n",
    "init_weights(gen_Z)\n",
    "init_weights(gen_P)\n",
    "init_weights(disc_Z)\n",
    "init_weights(disc_P)\n",
    "\n",
    "# Optimizers\n",
    "opt_gen = optim.Adam(\n",
    "    list(gen_Z.parameters()) + list(gen_P.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "opt_disc = optim.Adam(\n",
    "    list(disc_Z.parameters()) + list(disc_P.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "# Schedulers\n",
    "# Linear decay after 15 epochs\n",
    "def lambda_rule(epoch):\n",
    "    lr_l = 1.0 - max(0, epoch + 1 - 15) / float(15 + 1)\n",
    "    return lr_l\n",
    "scheduler_gen = optim.lr_scheduler.LambdaLR(opt_gen, lr_lambda=lambda_rule)\n",
    "scheduler_disc = optim.lr_scheduler.LambdaLR(opt_disc, lr_lambda=lambda_rule)\n",
    "\n",
    "# Losses\n",
    "L1 = nn.L1Loss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Buffers\n",
    "fake_monet_buffer = ReplayBuffer()\n",
    "fake_photo_buffer = ReplayBuffer()\n",
    "\n",
    "def train_fn(disc_Z, disc_P, gen_Z, gen_P, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for idx, (monet, photo) in enumerate(loop):\n",
    "        monet = monet.to(device)\n",
    "        photo = photo.to(device)\n",
    "\n",
    "        # Train Generators H and Z\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Identity loss\n",
    "            fake_monet = gen_Z(monet)\n",
    "            loss_identity_monet = L1(fake_monet, monet) * LAMBDA_IDENTITY\n",
    "            \n",
    "            fake_photo = gen_P(photo)\n",
    "            loss_identity_photo = L1(fake_photo, photo) * LAMBDA_IDENTITY\n",
    "\n",
    "            # GAN loss\n",
    "            fake_monet = gen_Z(photo)\n",
    "            D_Z_fake = disc_Z(fake_monet)\n",
    "            loss_GAN_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake))\n",
    "\n",
    "            fake_photo = gen_P(monet)\n",
    "            D_P_fake = disc_P(fake_photo)\n",
    "            loss_GAN_P = mse(D_P_fake, torch.ones_like(D_P_fake))\n",
    "\n",
    "            # Cycle loss\n",
    "            cycle_monet = gen_Z(fake_photo)\n",
    "            loss_cycle_monet = L1(cycle_monet, monet) * LAMBDA_CYCLE\n",
    "\n",
    "            cycle_photo = gen_P(fake_monet)\n",
    "            loss_cycle_photo = L1(cycle_photo, photo) * LAMBDA_CYCLE\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = (\n",
    "                loss_GAN_Z\n",
    "                + loss_GAN_P\n",
    "                + loss_cycle_monet\n",
    "                + loss_cycle_photo\n",
    "                + loss_identity_monet\n",
    "                + loss_identity_photo\n",
    "            )\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(loss_G).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        # Train Discriminators H and Z\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Discriminator P\n",
    "            D_P_real = disc_P(photo)\n",
    "            loss_D_P_real = mse(D_P_real, torch.ones_like(D_P_real))\n",
    "            \n",
    "            fake_photo_ = fake_photo_buffer.push_and_pop(fake_photo)\n",
    "            D_P_fake = disc_P(fake_photo_.detach())\n",
    "            loss_D_P_fake = mse(D_P_fake, torch.zeros_like(D_P_fake))\n",
    "            loss_D_P = (loss_D_P_real + loss_D_P_fake) / 2\n",
    "\n",
    "            # Discriminator Z\n",
    "            D_Z_real = disc_Z(monet)\n",
    "            loss_D_Z_real = mse(D_Z_real, torch.ones_like(D_Z_real))\n",
    "            \n",
    "            fake_monet_ = fake_monet_buffer.push_and_pop(fake_monet)\n",
    "            D_Z_fake = disc_Z(fake_monet_.detach())\n",
    "            loss_D_Z_fake = mse(D_Z_fake, torch.zeros_like(D_Z_fake))\n",
    "            loss_D_Z = (loss_D_Z_real + loss_D_Z_fake) / 2\n",
    "\n",
    "            # Total loss\n",
    "            loss_D = (loss_D_P + loss_D_Z) / 2\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        d_scaler.scale(loss_D).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        if idx % 200 == 0:\n",
    "            loop.set_postfix(loss_G=loss_G.item(), loss_D=loss_D.item())\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "# Training\n",
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    train_fn(disc_Z, disc_P, gen_Z, gen_P, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler)\n",
    "    \n",
    "    # Step schedulers\n",
    "    scheduler_gen.step()\n",
    "    scheduler_disc.step()\n",
    "    \n",
    "    # Save checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_checkpoint(gen_Z, opt_gen, filename=f\"gen_Z_{epoch+1}.pth.tar\")\n",
    "        save_checkpoint(gen_P, opt_gen, filename=f\"gen_P_{epoch+1}.pth.tar\")\n",
    "        \n",
    "    # Visualize progress\n",
    "    # (Optional: Add visualization code here to see generated images during training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342363b",
   "metadata": {},
   "source": [
    "## 5. Inference & Submission\n",
    "Generate Monet-style images from all photos and save them to a zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6317da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def generate_images(gen_Z, photo_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    gen_Z.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    print(f\"Generating images from {photo_dir}...\")\n",
    "    photo_files = os.listdir(photo_dir)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, file in enumerate(tqdm(photo_files)):\n",
    "            img_path = os.path.join(photo_dir, file)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = transform(img).unsqueeze(0).to(device)\n",
    "            \n",
    "            fake_monet = gen_Z(img)\n",
    "            \n",
    "            # Denormalize\n",
    "            fake_monet = fake_monet * 0.5 + 0.5\n",
    "            fake_monet = fake_monet.squeeze(0).cpu().detach()\n",
    "            \n",
    "            # Save\n",
    "            save_path = os.path.join(output_dir, file)\n",
    "            # Convert to PIL and save\n",
    "            transforms.ToPILImage()(fake_monet).save(save_path)\n",
    "            \n",
    "    print(\"Generation complete.\")\n",
    "\n",
    "# Generate\n",
    "output_dir = 'images'\n",
    "photo_dir = os.path.join(dataset_dir, 'photo_jpg')\n",
    "generate_images(gen_Z, photo_dir, output_dir)\n",
    "\n",
    "# Zip\n",
    "shutil.make_archive('images', 'zip', output_dir)\n",
    "print(\"Images zipped successfully. Ready for submission!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
