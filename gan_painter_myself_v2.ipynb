{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c01190c",
   "metadata": {},
   "source": [
    "# I'm Something of a Painter Myself - CycleGAN Implementation\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### The Problem: Style Transfer with Unpaired Data\n",
    "The goal of this project is to perform **Image-to-Image Translation**, specifically transforming ordinary landscape photos into paintings in the style of **Claude Monet**. \n",
    "\n",
    "In many computer vision tasks, we have \"paired\" data (e.g., a photo of a shoe and a sketch of the exact same shoe). However, for this task, we do not have pairs of \"a photo of a place\" and \"a Monet painting of that exact same place\". We only have two independent collections:\n",
    "*   **Domain A**: A set of ~7000 landscape photos.\n",
    "*   **Domain B**: A set of ~300 Monet paintings.\n",
    "\n",
    "This is where **CycleGAN** comes in. It is designed to learn a mapping between two domains without requiring paired training data. It achieves this by enforcing **Cycle Consistency**: if we translate a photo to a painting and then back to a photo, we should get the original photo back.\n",
    "\n",
    "### The Competition: \"I'm Something of a Painter Myself\"\n",
    "This notebook is designed for the Kaggle competition [I'm Something of a Painter Myself](https://www.kaggle.com/competitions/gan-getting-started/overview).\n",
    "\n",
    "*   **Objective**: Build a GAN model that generates 7,000 to 10,000 Monet-style images from the provided photo dataset.\n",
    "*   **Evaluation Metric**: **MiFID (Memorization-informed FrÃ©chet Inception Distance)**. \n",
    "    *   Lower MiFID is better.\n",
    "    *   It measures both the quality/diversity of generated images (FID) and penalizes the model if it simply memorizes the training set (Memorization distance).\n",
    "\n",
    "### Project Overview & Strategy\n",
    "We will implement a CycleGAN from scratch using **PyTorch**. The workflow includes:\n",
    "1.  **Data Pipeline**: Efficiently loading data from Google Drive (for Colab) or Kaggle input.\n",
    "2.  **Model Architecture**: \n",
    "    *   **Generator**: ResNet-based architecture to transform images.\n",
    "    *   **Discriminator**: PatchGAN to classify real vs. fake image patches.\n",
    "3.  **Training**: Using Adversarial Loss, Cycle Consistency Loss, and Identity Loss.\n",
    "4.  **Inference**: Generating the final set of images for submission.\n",
    "\n",
    "**Environment**: This notebook is designed to run seamlessly on **Google Colab (Pro)**, **Kaggle Kernels**, or a **Local Machine**. It automatically detects the environment to configure paths and hyperparameters (like batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "import sys\n",
    "import gdown\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7881e",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Visual EDA\n",
    "\n",
    "### Dataset Overview\n",
    "The dataset for this project is sourced from the Kaggle competition. It consists of two distinct domains:\n",
    "\n",
    "| Domain | Folder Name | Description | Count |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Source (Domain A)** | `photo_jpg` | Real-world landscape photos. | ~7,038 images |\n",
    "| **Target (Domain B)** | `monet_jpg` | Paintings by Claude Monet. | 300 images |\n",
    "\n",
    "**Key Characteristics**:\n",
    "*   **Unpaired**: There is no one-to-one mapping between a specific photo and a specific painting.\n",
    "*   **Imbalanced**: We have significantly more photos than paintings (Ratio ~23:1). This imbalance makes data augmentation and stable training crucial.\n",
    "\n",
    "### Data Loading Strategy\n",
    "We implement an environment-aware loading strategy:\n",
    "1.  **Colab**: Downloads the dataset from a shared Google Drive link (via `gdown`) and unzips it locally for high-speed I/O.\n",
    "2.  **Kaggle**: Directly accesses the read-only dataset provided by the platform.\n",
    "3.  **Local**: Downloads and unzips to the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6fbdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Detect Environment\n",
    "def get_env():\n",
    "    if 'google.colab' in sys.modules:\n",
    "        return 'colab'\n",
    "    elif 'kaggle_web_client' in sys.modules or os.path.exists('/kaggle'):\n",
    "        return 'kaggle'\n",
    "    else:\n",
    "        return 'local'\n",
    "\n",
    "ENV = get_env()\n",
    "print(f\"Running in {ENV} environment.\")\n",
    "\n",
    "# 2. Define paths based on environment\n",
    "if ENV == 'colab':\n",
    "    # Shared Link: https://drive.google.com/file/d/1Wf8cZM1QboZamZDoL9hcuA8yIFUQtgEb/view?usp=sharing\n",
    "    file_id = '1Wf8cZM1QboZamZDoL9hcuA8yIFUQtgEb'\n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    \n",
    "    base_dir = '/content'\n",
    "    local_zip_path = os.path.join(base_dir, 'gan-getting-started.zip')\n",
    "    dataset_dir = os.path.join(base_dir, 'dataset')\n",
    "    \n",
    "    # Download and Unzip if needed\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        print(f\"Downloading zip file to {local_zip_path}...\")\n",
    "        gdown.download(url, local_zip_path, quiet=False)\n",
    "        \n",
    "        print(\"Unzipping...\")\n",
    "        shutil.unpack_archive(local_zip_path, dataset_dir)\n",
    "        print(\"Done!\")\n",
    "    else:\n",
    "        print(\"Dataset already exists.\")\n",
    "\n",
    "elif ENV == 'kaggle':\n",
    "    # Kaggle standard input directory\n",
    "    dataset_dir = '/kaggle/input/gan-getting-started'\n",
    "    print(f\"Using Kaggle dataset at {dataset_dir}\")\n",
    "\n",
    "else: # Local\n",
    "    # Assume local setup or download\n",
    "    file_id = '1Wf8cZM1QboZamZDoL9hcuA8yIFUQtgEb'\n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    \n",
    "    base_dir = '.'\n",
    "    local_zip_path = os.path.join(base_dir, 'gan-getting-started.zip')\n",
    "    dataset_dir = os.path.join(base_dir, 'dataset')\n",
    "    \n",
    "    if not os.path.exists(dataset_dir):\n",
    "        print(f\"Downloading zip file to {local_zip_path}...\")\n",
    "        gdown.download(url, local_zip_path, quiet=False)\n",
    "        print(\"Unzipping...\")\n",
    "        shutil.unpack_archive(local_zip_path, dataset_dir)\n",
    "        print(\"Done!\")\n",
    "\n",
    "# 3. Verify and Visualize Distribution\n",
    "if os.path.exists(dataset_dir):\n",
    "    photo_dir = os.path.join(dataset_dir, 'photo_jpg')\n",
    "    monet_dir = os.path.join(dataset_dir, 'monet_jpg')\n",
    "    \n",
    "    if os.path.exists(photo_dir) and os.path.exists(monet_dir):\n",
    "        n_photo = len(os.listdir(photo_dir))\n",
    "        n_monet = len(os.listdir(monet_dir))\n",
    "        \n",
    "        print(f\"Photos: {n_photo}\")\n",
    "        print(f\"Monet Paintings: {n_monet}\")\n",
    "        \n",
    "        # Bar Chart Visualization\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        bars = plt.bar(['Photos (Source)', 'Monet (Target)'], [n_photo, n_monet], color=['#4a90e2', '#f5a623'])\n",
    "        plt.title('Dataset Distribution: Photos vs. Paintings', fontsize=14)\n",
    "        plt.ylabel('Number of Images')\n",
    "        \n",
    "        # Add counts on top of bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 100,\n",
    "                     f'{height}', ha='center', va='bottom', fontsize=12)\n",
    "        \n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Directories not found. Check path structure.\")\n",
    "else:\n",
    "    print(f\"Dataset directory {dataset_dir} not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c40ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_monet, root_photo, transform=None):\n",
    "        self.root_monet = root_monet\n",
    "        self.root_photo = root_photo\n",
    "        self.transform = transform\n",
    "\n",
    "        self.monet_images = os.listdir(root_monet)\n",
    "        self.photo_images = os.listdir(root_photo)\n",
    "        self.length_dataset = max(len(self.monet_images), len(self.photo_images))\n",
    "        self.monet_len = len(self.monet_images)\n",
    "        self.photo_len = len(self.photo_images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        monet_img = self.monet_images[index % self.monet_len]\n",
    "        photo_img = self.photo_images[index % self.photo_len]\n",
    "\n",
    "        monet_path = os.path.join(self.root_monet, monet_img)\n",
    "        photo_path = os.path.join(self.root_photo, photo_img)\n",
    "\n",
    "        monet_img = np.array(Image.open(monet_path).convert(\"RGB\"))\n",
    "        photo_img = np.array(Image.open(photo_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.transform:\n",
    "            monet_img = Image.fromarray(monet_img)\n",
    "            photo_img = Image.fromarray(photo_img)\n",
    "            monet_img = self.transform(monet_img)\n",
    "            photo_img = self.transform(photo_img)\n",
    "\n",
    "        return monet_img, photo_img\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = ImageDataset(\n",
    "    root_monet=os.path.join(dataset_dir, 'monet_jpg'),\n",
    "    root_photo=os.path.join(dataset_dir, 'photo_jpg'),\n",
    "    transform=transform\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Visualization\n",
    "def show_sample(loader):\n",
    "    monet, photo = next(iter(loader))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    # Denormalize\n",
    "    monet = monet * 0.5 + 0.5\n",
    "    photo = photo * 0.5 + 0.5\n",
    "\n",
    "    ax[0].imshow(monet[0].permute(1, 2, 0))\n",
    "    ax[0].set_title(\"Monet Style\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    ax[1].imshow(photo[0].permute(1, 2, 0))\n",
    "    ax[1].set_title(\"Photo\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_sample(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc8e581",
   "metadata": {},
   "source": [
    "## 3. Model Architecture (CycleGAN)\n",
    "\n",
    "### What is CycleGAN?\n",
    "CycleGAN (Cycle-Consistent Adversarial Networks) is a technique for training unsupervised image-to-image translation models. Unlike traditional GANs that require paired training data (e.g., a specific photo and its corresponding painting), CycleGAN learns to translate between two domains (e.g., Domain X: Photos, Domain Y: Monet Paintings) using **unpaired** data.\n",
    "\n",
    "It achieves this by training two sets of Generators and Discriminators simultaneously:\n",
    "*   **Generator $G$**: Translates $X \\rightarrow Y$ (Photo to Monet).\n",
    "*   **Generator $F$**: Translates $Y \\rightarrow X$ (Monet to Photo).\n",
    "*   **Discriminator $D_Y$**: Distinguishes real images in $Y$ from generated images $G(X)$.\n",
    "*   **Discriminator $D_X$**: Distinguishes real images in $X$ from generated images $F(Y)$.\n",
    "\n",
    "### Why CycleGAN for this Project?\n",
    "1.  **Unpaired Data**: The competition provides a set of photos and a separate set of Monet paintings, but no one-to-one mapping exists. CycleGAN is the state-of-the-art solution for this specific constraint.\n",
    "2.  **Cycle Consistency**: To prevent the model from hallucinating or ignoring the input image entirely, CycleGAN enforces that $F(G(x)) \\approx x$. This ensures that if we turn a photo into a painting and back, we recover the original photo, preserving the structural content (trees, buildings, mountains) while only changing the style.\n",
    "3.  **Identity Loss**: We also use identity mapping ($G(y) \\approx y$) to preserve color composition when the input already looks like the target domain.\n",
    "\n",
    "### Architecture Components\n",
    "We implement the standard architecture proposed in the original paper:\n",
    "1.  **Generator**: ResNet-based architecture (9 residual blocks for 256x256 images). ResNet is preferred over U-Net here because we want to transform the style while keeping the spatial structure intact, and residual connections help preserve information across deep layers.\n",
    "2.  **Discriminator**: PatchGAN (70x70). Instead of classifying the whole image as real/fake, it classifies overlapping $70 \\times 70$ patches. This encourages high-frequency \"texture\" realism.\n",
    "3.  **Replay Buffer**: A history of generated images is used to update the discriminator, preventing it from overfitting to the most recent generator output (reducing oscillation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d025a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels=3, num_residuals=9):\n",
    "        super(Generator, self).__init__()\n",
    "        # Initial Convolution\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(img_channels, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # Residual Blocks\n",
    "        for _ in range(num_residuals):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # Output Layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, img_channels, 7),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], 4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, feature, 4, stride=1 if feature == features[-1] else 2, padding=1, padding_mode=\"reflect\"),\n",
    "                    nn.InstanceNorm2d(feature),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                )\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        layers.append(nn.Conv2d(in_channels, 1, 4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(self.initial(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe13532",
   "metadata": {},
   "source": [
    "## 4. Training Configuration\n",
    "\n",
    "This section sets up the critical hyperparameters and initialization strategies for training. \n",
    "\n",
    "### Configuration Guide for Developers\n",
    "If you are forking this notebook or running it on different hardware, here is how you should adjust the settings:\n",
    "\n",
    "1.  **Batch Size (`BATCH_SIZE`)**:\n",
    "    *   **High-End GPUs (A100, V100)**: You can likely use `BATCH_SIZE = 32` or higher.\n",
    "    *   **Mid-Range GPUs (T4, P100 - Kaggle/Colab Standard)**: Use `BATCH_SIZE = 4` or `1`. CycleGAN is memory-intensive because it keeps 4 models (2 Generators, 2 Discriminators) in memory.\n",
    "    *   **CPU**: Set `BATCH_SIZE = 1`. Training will be extremely slow.\n",
    "\n",
    "2.  **Learning Rate (`LEARNING_RATE`)**:\n",
    "    *   Standard CycleGAN uses `2e-4`.\n",
    "    *   We use a linear decay scheduler that keeps the rate constant for the first half of training and decays it to zero over the second half.\n",
    "\n",
    "3.  **Epochs (`NUM_EPOCHS`)**:\n",
    "    *   **Debugging**: Set to `1-5` to ensure code runs.\n",
    "    *   **Good Results**: `30-50` epochs.\n",
    "    *   **Best Results**: `100-200` epochs (requires several hours on a GPU).\n",
    "\n",
    "4.  **Loss Weights**:\n",
    "    *   `LAMBDA_CYCLE = 10`: Controls how strictly the model enforces $F(G(x)) \\approx x$. Higher values preserve more structure (edges, shapes) but might limit style transfer.\n",
    "    *   `LAMBDA_IDENTITY = 0.5`: Controls color preservation.\n",
    "\n",
    "### Components\n",
    "*   **Weights Initialization**: We use a Normal distribution ($\\mu=0, \\sigma=0.02$) as recommended in the paper.\n",
    "*   **Replay Buffer**: Stores the last 50 generated images. This is crucial for stabilizing the Discriminator, preventing it from oscillating by \"forgetting\" previous fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18edecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                nn.init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                nn.init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd71ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Configure Batch Size based on Environment\n",
    "if ENV == 'colab':\n",
    "    BATCH_SIZE = 32 # A100 can handle 32\n",
    "    NUM_WORKERS = 4\n",
    "elif ENV == 'kaggle':\n",
    "    BATCH_SIZE = 4 # P100/T4 on Kaggle might struggle with 32. 4 is safe.\n",
    "    NUM_WORKERS = 2\n",
    "else:\n",
    "    BATCH_SIZE = 1\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "print(f\"Configuration: ENV={ENV}, BATCH_SIZE={BATCH_SIZE}, NUM_WORKERS={NUM_WORKERS}\")\n",
    "\n",
    "NUM_EPOCHS = 30 # Increase this for better results (e.g., 30-50)\n",
    "LAMBDA_CYCLE = 10\n",
    "LAMBDA_IDENTITY = 0.5\n",
    "\n",
    "# Re-create DataLoader to apply BATCH_SIZE and parallel loading\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Initialize Models\n",
    "gen_Z = Generator(img_channels=3, num_residuals=9).to(device) # Photo -> Monet\n",
    "gen_P = Generator(img_channels=3, num_residuals=9).to(device) # Monet -> Photo\n",
    "disc_Z = Discriminator(in_channels=3).to(device) # Classify Monet\n",
    "disc_P = Discriminator(in_channels=3).to(device) # Classify Photo\n",
    "\n",
    "# Initialize Weights\n",
    "init_weights(gen_Z)\n",
    "init_weights(gen_P)\n",
    "init_weights(disc_Z)\n",
    "init_weights(disc_P)\n",
    "\n",
    "# Optimizers\n",
    "opt_gen = optim.Adam(\n",
    "    list(gen_Z.parameters()) + list(gen_P.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "opt_disc = optim.Adam(\n",
    "    list(disc_Z.parameters()) + list(disc_P.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "# Schedulers\n",
    "# Linear decay after 15 epochs\n",
    "def lambda_rule(epoch):\n",
    "    lr_l = 1.0 - max(0, epoch + 1 - 15) / float(15 + 1)\n",
    "    return lr_l\n",
    "scheduler_gen = optim.lr_scheduler.LambdaLR(opt_gen, lr_lambda=lambda_rule)\n",
    "scheduler_disc = optim.lr_scheduler.LambdaLR(opt_disc, lr_lambda=lambda_rule)\n",
    "\n",
    "# Losses\n",
    "L1 = nn.L1Loss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Buffers\n",
    "fake_monet_buffer = ReplayBuffer()\n",
    "fake_photo_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a65276",
   "metadata": {},
   "source": [
    "## 5. Training & Monitoring\n",
    "\n",
    "### Training Strategy\n",
    "The training loop for CycleGAN is complex because we are training **four networks** simultaneously ($G$, $F$, $D_X$, $D_Y$).\n",
    "\n",
    "**Step 1: Train Generators ($G$ and $F$)**\n",
    "We update the generators to minimize a weighted sum of three losses:\n",
    "1.  **Adversarial Loss**: $G$ tries to fool $D_Y$ (make generated Monet look real). $F$ tries to fool $D_X$.\n",
    "2.  **Cycle Consistency Loss**: $F(G(x)) \\approx x$ and $G(F(y)) \\approx y$. This ensures the image content is preserved.\n",
    "3.  **Identity Loss**: $G(y) \\approx y$ and $F(x) \\approx x$. This preserves color and prevents the model from making unnecessary changes if the image is already in the target domain.\n",
    "\n",
    "**Step 2: Train Discriminators ($D_X$ and $D_Y$)**\n",
    "We update the discriminators to correctly classify:\n",
    "*   Real images as **Real** (label 1).\n",
    "*   Generated images as **Fake** (label 0).\n",
    "*   *Note*: We use the **Replay Buffer** here. Instead of always showing the discriminator the *latest* generated image, we sometimes show it an image generated a few steps ago. This stabilizes training.\n",
    "\n",
    "### Performance Optimization\n",
    "*   **Mixed Precision (`torch.amp`)**: We use Automatic Mixed Precision. This casts some operations to `float16` instead of `float32`, significantly reducing memory usage and speeding up training on modern GPUs (T4, P100, V100, A100).\n",
    "*   **GradScaler**: Manages gradient scaling to prevent underflow when using mixed precision.\n",
    "\n",
    "### Monitoring\n",
    "*   **Tqdm Progress Bar**: Shows real-time progress for each epoch.\n",
    "*   **Loss Logging**: We print `loss_G` (Generator total loss) and `loss_D` (Discriminator total loss) periodically.\n",
    "*   **Visual Validation**: At the end of every epoch, we run a fixed \"test photo\" through the generator and display the result. This allows us to visually confirm that the model is learning the Monet style and not just collapsing to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f695fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(disc_Z, disc_P, gen_Z, gen_P, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for idx, (monet, photo) in enumerate(loop):\n",
    "        monet = monet.to(device)\n",
    "        photo = photo.to(device)\n",
    "\n",
    "        # Train Generators H and Z\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # Identity loss\n",
    "            fake_monet = gen_Z(monet)\n",
    "            loss_identity_monet = L1(fake_monet, monet) * LAMBDA_IDENTITY\n",
    "            \n",
    "            fake_photo = gen_P(photo)\n",
    "            loss_identity_photo = L1(fake_photo, photo) * LAMBDA_IDENTITY\n",
    "\n",
    "            # GAN loss\n",
    "            fake_monet = gen_Z(photo)\n",
    "            D_Z_fake = disc_Z(fake_monet)\n",
    "            loss_GAN_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake))\n",
    "\n",
    "            fake_photo = gen_P(monet)\n",
    "            D_P_fake = disc_P(fake_photo)\n",
    "            loss_GAN_P = mse(D_P_fake, torch.ones_like(D_P_fake))\n",
    "\n",
    "            # Cycle loss\n",
    "            cycle_monet = gen_Z(fake_photo)\n",
    "            loss_cycle_monet = L1(cycle_monet, monet) * LAMBDA_CYCLE\n",
    "\n",
    "            cycle_photo = gen_P(fake_monet)\n",
    "            loss_cycle_photo = L1(cycle_photo, photo) * LAMBDA_CYCLE\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = (\n",
    "                loss_GAN_Z\n",
    "                + loss_GAN_P\n",
    "                + loss_cycle_monet\n",
    "                + loss_cycle_photo\n",
    "                + loss_identity_monet\n",
    "                + loss_identity_photo\n",
    "            )\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(loss_G).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        # Train Discriminators H and Z\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # Discriminator P\n",
    "            D_P_real = disc_P(photo)\n",
    "            loss_D_P_real = mse(D_P_real, torch.ones_like(D_P_real))\n",
    "            \n",
    "            fake_photo_ = fake_photo_buffer.push_and_pop(fake_photo)\n",
    "            D_P_fake = disc_P(fake_photo_.detach())\n",
    "            loss_D_P_fake = mse(D_P_fake, torch.zeros_like(D_P_fake))\n",
    "            loss_D_P = (loss_D_P_real + loss_D_P_fake) / 2\n",
    "\n",
    "            # Discriminator Z\n",
    "            D_Z_real = disc_Z(monet)\n",
    "            loss_D_Z_real = mse(D_Z_real, torch.ones_like(D_Z_real))\n",
    "            \n",
    "            fake_monet_ = fake_monet_buffer.push_and_pop(fake_monet)\n",
    "            D_Z_fake = disc_Z(fake_monet_.detach())\n",
    "            loss_D_Z_fake = mse(D_Z_fake, torch.zeros_like(D_Z_fake))\n",
    "            loss_D_Z = (loss_D_Z_real + loss_D_Z_fake) / 2\n",
    "\n",
    "            # Total loss\n",
    "            loss_D = (loss_D_P + loss_D_Z) / 2\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        d_scaler.scale(loss_D).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        if idx % 200 == 0:\n",
    "            loop.set_postfix(loss_G=loss_G.item(), loss_D=loss_D.item())\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def visualize_progress(gen, test_input, epoch):\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        fake = gen(test_input)\n",
    "        fake = fake * 0.5 + 0.5 # Denormalize\n",
    "        fake = fake.cpu()\n",
    "        \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(fake[0].permute(1, 2, 0))\n",
    "        plt.title(f\"Epoch {epoch+1} Generated Monet\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    gen.train()\n",
    "\n",
    "# Get a fixed sample for visualization\n",
    "fixed_monet, fixed_photo = next(iter(loader))\n",
    "fixed_photo = fixed_photo.to(device)\n",
    "\n",
    "# Training Loop\n",
    "g_scaler = torch.amp.GradScaler('cuda')\n",
    "d_scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    train_fn(disc_Z, disc_P, gen_Z, gen_P, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler)\n",
    "    \n",
    "    # Visualization\n",
    "    visualize_progress(gen_Z, fixed_photo, epoch)\n",
    "    \n",
    "    # Step schedulers\n",
    "    scheduler_gen.step()\n",
    "    scheduler_disc.step()\n",
    "    \n",
    "    # Save checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_checkpoint(gen_Z, opt_gen, filename=f\"gen_Z_{epoch+1}.pth.tar\")\n",
    "        save_checkpoint(gen_P, opt_gen, filename=f\"gen_P_{epoch+1}.pth.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a9e3c",
   "metadata": {},
   "source": [
    "## 6. Inference & Submission\n",
    "\n",
    "Once training is complete, we use the trained Generator `gen_Z` (Photo -> Monet) to transform all images in the `photo_jpg` directory.\n",
    "The images are saved to a directory and then zipped for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e710ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(gen_Z, photo_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    gen_Z.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    print(f\"Generating images from {photo_dir}...\")\n",
    "    photo_files = os.listdir(photo_dir)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, file in enumerate(tqdm(photo_files)):\n",
    "            img_path = os.path.join(photo_dir, file)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "            fake_monet = gen_Z(img)\n",
    "\n",
    "            # Denormalize\n",
    "            fake_monet = fake_monet * 0.5 + 0.5\n",
    "            fake_monet = fake_monet.squeeze(0).cpu().detach()\n",
    "\n",
    "            # Save\n",
    "            save_path = os.path.join(output_dir, file)\n",
    "            # Convert to PIL and save\n",
    "            transforms.ToPILImage()(fake_monet).save(save_path)\n",
    "\n",
    "    print(\"Generation complete.\")\n",
    "\n",
    "# Generate\n",
    "output_dir = 'images'\n",
    "photo_dir = os.path.join(dataset_dir, 'photo_jpg')\n",
    "generate_images(gen_Z, photo_dir, output_dir)\n",
    "\n",
    "# Zip\n",
    "shutil.make_archive('images', 'zip', output_dir)\n",
    "print(\"Images zipped successfully. Ready for submission!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028fee0c",
   "metadata": {},
   "source": [
    "## 7. Conclusion & Future Work\n",
    "\n",
    "### Conclusion\n",
    "We successfully implemented a CycleGAN to translate landscape photos into Monet-style paintings. \n",
    "*   The model learned to capture the texture and color palette of Monet's work.\n",
    "*   The use of Cycle Consistency Loss ensured that the structural content of the original photos was preserved.\n",
    "\n",
    "### Future Work\n",
    "To improve the MiFID score, we could consider:\n",
    "1.  **Data Augmentation**: Using more advanced augmentations (e.g., random crops, flips) to increase data diversity.\n",
    "2.  **Hyperparameter Tuning**: Experimenting with different weights for the Identity and Cycle losses.\n",
    "3.  **Architecture Variants**: Trying U-Net based generators or different discriminator architectures.\n",
    "4.  **Longer Training**: Training for more epochs (e.g., 100+) with a decaying learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73fe27",
   "metadata": {},
   "source": [
    "## 8. References\n",
    "\n",
    "1.  **CycleGAN Paper**: Zhu, J. Y., Park, T., Isola, P., & Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. *ICCV*.\n",
    "2.  **Kaggle Competition**: [I'm Something of a Painter Myself](https://www.kaggle.com/competitions/gan-getting-started)\n",
    "3.  **PyTorch Documentation**: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
